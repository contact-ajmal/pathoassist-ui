{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PathoAssist: The \"Under-the-Hood\" Tour \\U0001f680\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
                "\n",
                "Welcome to the backend engine room of **PathoAssist**. While our interactive desktop app provides a seamless, \"Apple-style\" clinical experience, this notebook peels back the UI to show you exactly how our pipeline works.\n",
                "\n",
                "We will walk through the 4 core steps of the PathoAssist backend as they happen in the app:\n",
                "1. **The Viewer Phase:** Ingesting a histological sample.\n",
                "2. **The Preprocessing Phase:** Our smart Otsu thresholding that discards useless glass background.\n",
                "3. **The Inference Engine:** Loading MedGemma with clinical RAG on edge hardware.\n",
                "4. **The PathoCopilot:** Simulating our interactive chatbot layer.\n",
                "\n",
                "> **Note:** Ensure you are running this in a GPU runtime (Runtime > Change runtime type > T4 GPU) to simulate \"Edge\" consumer hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install core backend dependencies\n",
                "!pip install -q torch transformers pillow opencv-python matplotlib \"accelerate>=0.26.0\" bitsandbytes huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Authentication (MedGemma Access)\n",
                "Google's MedGemma models are gated. \n",
                "1. Ensure you have accepted the terms on [google/medgemma-2b](https://huggingface.co/google/medgemma-2b).\n",
                "2. Run the cell below to securely log in with your HuggingFace User Access Token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import notebook_login\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: The Viewer Phase (WSI Ingestion)\n",
                "In the PathoAssist app, a clinician drags and drops a massive gigapixel `.svs` WSI file. Our backend slices this into manageable tiles. \n",
                "\n",
                "For this interactive tour, we will download a **Real Clinical Sample** (an H&E stained histological patch of Colorectal Carcinoma) directly from a verified medical database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import cv2\n",
                "import urllib.request\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
                "\n",
                "# 1. Download an Actual Tissue Patch (Colorectal Carcinoma H&E Stain)\n",
                "image_url = \"https://upload.wikimedia.org/wikipedia/commons/e/ec/Colorectal_carcinoma%2C_H%26E_stain.jpg\"\n",
                "req = urllib.request.Request(image_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
                "with urllib.request.urlopen(req) as response:\n",
                "    with open(\"clinical_patch.jpg\", \"wb\") as f:\n",
                "        f.write(response.read())\n",
                "\n",
                "raw_image = Image.open(\"clinical_patch.jpg\").convert(\"RGB\")\n",
                "\n",
                "plt.figure(figsize=(6, 6))\n",
                "plt.imshow(raw_image)\n",
                "plt.title(\"Step 1: Raw Clinical Tissue Patch (H&E)\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Smart Tissue Detection (Preprocessing)\n",
                "WSIs contain up to 60% blank whitespace (glass). Feeding glass to MedGemma is a massive waste of compute resource. \n",
                "\n",
                "Here, we demonstrate the exact `tiling.py` logic used in PathoAssist. We apply **Otsu's Thresholding** to create a binary mask, proving mathematically to the engine whether the patch is worth analyzing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_tissue_detection(image: Image.Image):\n",
                "    img_array = np.array(image)\n",
                "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
                "    \n",
                "    # PathoAssist Core Logic: Otsu's Thresholding\n",
                "    # Automatically finds the optimal separation between dark tissue and bright glass\n",
                "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
                "    \n",
                "    tissue_pixels = np.sum(binary < 200)\n",
                "    tissue_ratio = tissue_pixels / binary.size\n",
                "    \n",
                "    # Visualizing the \"Under the Hood\" Process\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "    ax1.imshow(gray, cmap='gray')\n",
                "    ax1.set_title(\"Grayscale Conversion\")\n",
                "    ax1.axis('off')\n",
                "    \n",
                "    # Invert binary for visualization (White = Human Tissue)\n",
                "    mask_viz = np.where(binary < 200, 255, 0)\n",
                "    ax2.imshow(mask_viz, cmap='magma')\n",
                "    ax2.set_title(f\"AI Tissue Mask (Density: {tissue_ratio*100:.1f}%)\")\n",
                "    ax2.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "    if tissue_ratio > 0.15:\n",
                "        print(\"\\u2705 Status: Valid Tissue. Pushing to MedGemma.\")\n",
                "    return tissue_ratio\n",
                "\n",
                "density = demonstrate_tissue_detection(raw_image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: The Inference Engine (MedGemma)\n",
                "Now we load Google's HAI-DEF MedGemma model. \n",
                "\n",
                "**The Edge Feasibility Test:** To prove PathoAssist works in rural clinics without expensive servers, we load the model in **4-bit dynamic quantized mode**. This shrinks a massively powerful LLM so it fits comfortably inside standard laptop RAM (simulated here by Colab's T4 GPU)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_ID = \"google/medgemma-2b\" \n",
                "\n",
                "print(\"Loading MedGemma via bitsandbytes (4-bit)...\\n\")\n",
                "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
                "model = AutoModelForImageTextToText.from_pretrained(\n",
                "    MODEL_ID, \n",
                "    device_map=\"auto\",\n",
                "    load_in_4bit=True, # Crucial for Edge deployment\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "print(\"\\u2714\\ufe0f Engine Initialized on Edge equivalent hardware.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clinical RAG (Retrieval-Augmented Generation)\n",
                "PathoAssist doesn't just do image classification. It does **Clinical Reasoning**. We inject the patient's EHR (Electronic Health Record) directly into the multimodal payload, forcing MedGemma to interpret the image *through the lens* of the patient's history."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_patch_with_rag(processor, model, image, clinical_context):\n",
                "    img_resized = image.resize((224, 224), Image.Resampling.LANCZOS)\n",
                "    \n",
                "    # Replicating backend/app/inference/engine.py payload construction\n",
                "    text_prompt = f\"Based on this pathology patch and the patient's clinical history: ({clinical_context}), describe the morphological findings. Be specific and act as an expert pathologist.\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": [\n",
                "            {\"type\": \"image\"},\n",
                "            {\"type\": \"text\", \"text\": text_prompt}\n",
                "        ]}\n",
                "    ]\n",
                "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
                "    inputs = processor(text=text, images=[img_resized], return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    print(\"\\u2699\\ufe0f Running Multimodal Inference...\")\n",
                "    with torch.no_grad():\n",
                "        generation = model.generate(**inputs, max_new_tokens=256, temperature=0.2, do_sample=True)\n",
                "    \n",
                "    # Decode response\n",
                "    generated_tokens = generation[0][inputs['input_ids'].shape[1]:]\n",
                "    report = processor.decode(generated_tokens, skip_special_tokens=True)\n",
                "    \n",
                "    return report, messages # We return messages to maintain chat state for Step 4\n",
                "\n",
                "mock_ehr = \"Patient: 62M. History of altered bowel habits and weight loss. Colonoscopy revealed a fungating mass in the ascending colon. Biopsy performed.\"\n",
                "print(f\"\\n\\U0001f4cb Injected EHR Context: {mock_ehr}\\n\")\n",
                "\n",
                "ai_report, chat_history = analyze_patch_with_rag(processor, model, raw_image, mock_ehr)\n",
                "\n",
                "print(\"===============================\")\n",
                "print(\"\\u2728 PathoAssist Preliminary Report \\u2728\")\n",
                "print(\"===============================\")\n",
                "print(ai_report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: The PathoCopilot (Interactive Chat)\n",
                "The most powerful UI feature of PathoAssist is the interactive Copilot. If a doctor disagrees or wants clarification, they don't have to guess. They can debate the model. \n",
                "\n",
                "Because we maintain state in our backend architecture, the model remembers the image and its previous report."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_copilot_interaction(processor, model, chat_history, image, doctor_question):\n",
                "    img_resized = image.resize((224, 224), Image.Resampling.LANCZOS)\n",
                "    \n",
                "    # Append the AI's previous answer to the history\n",
                "    chat_history.append({\"role\": \"assistant\", \"content\": ai_report})\n",
                "    \n",
                "    # Append the Doctor's new question\n",
                "    chat_history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": doctor_question}]})\n",
                "    \n",
                "    text = processor.apply_chat_template(chat_history, add_generation_prompt=True)\n",
                "    inputs = processor(text=text, images=[img_resized], return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    print(f\"\\U0001f468\\u200d\\u2695\\ufe0f Doctor asks: \\\"{doctor_question}\\\"\\n\")\n",
                "    print(\"\\u2699\\ufe0f Copilot is thinking...\\n\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        generation = model.generate(**inputs, max_new_tokens=256, temperature=0.3, do_sample=True)\n",
                "        \n",
                "    generated_tokens = generation[0][inputs['input_ids'].shape[1]:]\n",
                "    answer = processor.decode(generated_tokens, skip_special_tokens=True)\n",
                "    \n",
                "    print(\"\\U0001f916 Copilot Replies:\")\n",
                "    print(answer)\n",
                "\n",
                "# Simulating a real clinical workflow interaction\n",
                "simulate_copilot_interaction(\n",
                "    processor, \n",
                "    model, \n",
                "    chat_history, \n",
                "    raw_image, \n",
                "    \"Are there any architectural patterns like irregular glands or mucin production visible here?\"\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}