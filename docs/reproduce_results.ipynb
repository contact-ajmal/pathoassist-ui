{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div align=\"center\">\n",
                "  <h1>ðŸ”¬ PathoAssist: AI-Powered \"Digital Fellow\" for Pathology</h1>\n",
                "  <h3>Reproducible Inference Engine Tour</h3>\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "Welcome to the **PathoAssist Reproducible Inference Tour**. \n",
                "\n",
                "PathoAssist is a medical-grade web application designed to run entirely **offline on edge devices** (like MacBooks), ensuring 100% patient data privacy. \n",
                "\n",
                "This notebook provides a guided, code-level walk-through of our Python FastAPI backend pipeline. You will see exactly how we:\n",
                "1. **Pre-process Whole Slide Images (WSIs)** using OpenCV to filter out useless background glass.\n",
                "2. **Construct a Clinical RAG Prompt** that injects the patient's Electronic Health Record (EHR) into the AI payload.\n",
                "3. **Run Google MedGemma (2B/4B)** locally to generate explainable, objective pathological findings.\n",
                "4. **Interact with the PathoAssist AI Bot** to debate AI findings while dynamically retaining conversational context.\n",
                "\n",
                "> **Note for Judges/Reviewers:** To execute this notebook, you need a Hugging Face account with access to the `google/medgemma-2b` model. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1. Environment Setup & Authentication\n",
                "\n",
                "Let's install the critical Python dependencies. In production, this matches our `backend/requirements.txt`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Core computer vision and AI dependencies\n",
                "!pip install -q git+https://github.com/huggingface/transformers.git accelerate bitsandbytes\n",
                "!pip install -q Pillow requests opencv-python-headless matplotlib numpy\n",
                "\n",
                "import cv2\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import requests\n",
                "from transformers import AutoProcessor, AutoModelForCausalLM\n",
                "\n",
                "# Authenticate with HuggingFace \n",
                "from huggingface_hub import notebook_login\n",
                "\n",
                "print(\"Please login with your Hugging Face token (make sure you have accepted the MedGemma terms of use):\")\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2. Smart Tissue Detection (Computer Vision)\n",
                "\n",
                "Whole Slide Images (WSIs) often contain 40-60% empty glass. Running AI inference on blank white space wastes precious compute cycles. \n",
                "\n",
                "Before any image reaches MedGemma, PathoAssist uses **OpenCV** to calculate the variance of the patch. If the patch is pure white (glass) or mostly blood artifact, it is discarded. Only high-variance **Regions of Interest (ROIs)** are kept."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 1. Load a sample pathology tile from the internet (simulating an OpenSlide extraction)\n",
                "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Nodular_melanoma_%282%29.jpg/640px-Nodular_melanoma_%282%29.jpg\"\n",
                "try:\n",
                "    image_req = requests.get(image_url, stream=True).raw\n",
                "    raw_patch = Image.open(image_req).convert(\"RGB\")\n",
                "    # Standardize image size for the Vision Encoder (224x224 is standard for many vision models)\n",
                "    patch_image = raw_patch.resize((224, 224))\n",
                "except Exception as e:\n",
                "    print(\"Error loading sample image. Check internet connection.\")\n",
                "\n",
                "# 2. Convert to OpenCV format (BGR)\n",
                "img_cv = np.array(patch_image)\n",
                "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
                "\n",
                "# 3. Convert to Grayscale for thresholding\n",
                "gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
                "\n",
                "# 4. Apply Gaussian blur to reduce high-frequency noise\n",
                "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
                "\n",
                "# 5. Apply Otsu's Thresholding to dynamically separate tissue from background glass\n",
                "_, tissue_mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
                "\n",
                "# Calculate tissue percentage\n",
                "tissue_percentage = (np.count_nonzero(tissue_mask) / tissue_mask.size) * 100\n",
                "\n",
                "# Visualization\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
                "ax1.imshow(patch_image)\n",
                "ax1.set_title(\"Original 224x224 Patch\")\n",
                "ax1.axis('off')\n",
                "\n",
                "ax2.imshow(tissue_mask, cmap='gray')\n",
                "ax2.set_title(f\"Otsu Tissue Mask ({tissue_percentage:.1f}% Tissue)\")\n",
                "ax2.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "if tissue_percentage > 15.0:\n",
                "    print(\"[PASS] Patch contains enough tissue. Sending to AI Pipeline...\")\n",
                "else:\n",
                "    print(\"[FAIL] Patch is mostly background glass. Discarding to save compute.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3. Initialize MedGemma Engine\n",
                "\n",
                "PathoAssist connects to a local, off-the-grid instance of Google MedGemma. In production, we utilize `bitsandbytes` 4-bit quantization, allowing the 2B (and soon 4B) multimodal models to fit entirely within the extremely limited VRAM of edge devices (8GB Macs or basic clinical PCs)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Automatically detect the best available hardware accelerator\n",
                "if torch.cuda.is_available():\n",
                "    device = \"cuda\"\n",
                "elif torch.backends.mps.is_available():\n",
                "    device = \"mps\"\n",
                "else:\n",
                "    device = \"cpu\"\n",
                "    \n",
                "print(f\"Initializing neural engine on {device}...\")\n",
                "\n",
                "model_id = \"google/medgemma-2b\"\n",
                "\n",
                "# Load processor (handles both image tokenization and text tokenization)\n",
                "processor = AutoProcessor.from_pretrained(model_id)\n",
                "\n",
                "# Load the multimodal causal language model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    device_map=device,\n",
                "    torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
                ")\n",
                "model.eval() # Locking model gradients for faster inference\n",
                "print(\"Google MedGemma Loaded and ready for clinical requests!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4. Clinical RAG Workflow\n",
                "\n",
                "A major limitation of traditional computer vision models is that they operate in a vacuum. A pathologist NEVER looks at a slide without knowing the patient's history.\n",
                "\n",
                "PathoAssist uses **Retrieval-Augmented Generation (RAG)** to inject simulated Electronic Health Record (EHR) data directly into the model's text prompt natively. This allows MedGemma to synthesize the visual tokens with the clinical history to provide *context-aware* reasoning, rather than blind classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Simulated EHR Payload (JSON from Frontend)\n",
                "patient_context = {\n",
                "    \"age\": 64,\n",
                "    \"gender\": \"Male\",\n",
                "    \"history\": \"Patient presents with an irregularly pigmented lesion on the left forearm, changing in size over 6 months.\",\n",
                "    \"tissue_type\": \"Skin excision\",\n",
                "    \"stain\": \"H&E\"\n",
                "}\n",
                "\n",
                "def construct_safety_prompt(context: dict) -> str:\n",
                "    \"\"\"\n",
                "    Constructs the specialized system prompt enforcing the 'Safety Layer' \n",
                "    and objective histopathological description.\n",
                "    \"\"\"\n",
                "    return f\"\"\"\n",
                "    You are an expert, highly meticulous pathologist analyzing an H&E stained pathology patch.\n",
                "    \n",
                "    PATIENT CLINICAL CONTEXT:\n",
                "    - Demographics: {context.get('age')}yo {context.get('gender')}\n",
                "    - Clinical History: {context.get('history')}\n",
                "    - Tissue Origin: {context.get('tissue_type')}\n",
                "    \n",
                "    TASK:\n",
                "    Analyze this image tile and write a structured, objective morphological description. \n",
                "    Describe the cellular architecture, nuclear features (e.g., pleomorphism, nucleoli), \n",
                "    and any signs of invasion or mitotic activity. \n",
                "    \n",
                "    RULES:\n",
                "    1. Be descriptive and analytical.\n",
                "    2. Do NOT make a definitive final diagnosis. Use phrasing like \"Features are highly suggestive of...\" \n",
                "    or \"Findings are consistent with...\". \n",
                "    3. End with a recommendation for next steps.\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5. Multimodal Inference (Generating the Report)\n",
                "We now pass the validated image patch and the RAG-augmented prompt through the model to retrieve the clinical findings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 1. Combine Image and System Prompt into the Chat Template\n",
                "prompt_text = construct_safety_prompt(patient_context)\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"type\": \"image\"},\n",
                "            {\"type\": \"text\", \"text\": prompt_text}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "final_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
                "\n",
                "# 2. Tokenize the visual and textual inputs\n",
                "inputs = processor(\n",
                "    text=final_prompt,\n",
                "    images=patch_image,\n",
                "    return_tensors=\"pt\"\n",
                ").to(device)\n",
                "\n",
                "print(\"\\n--- Running MedGemma Forward Pass ---\\n\")\n",
                "\n",
                "# 3. Execute inference with low temperature (clinical setting requires high factuality, low hallucination)\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=512,\n",
                "        temperature=0.1,\n",
                "        do_sample=True\n",
                "    )\n",
                "\n",
                "# 4. Decode and clean the output\n",
                "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Extract only the AI's response (removing the prompt input structure)\n",
                "response_start = generated_text.rfind(\"model\\n\") + 6\n",
                "first_pass_result = generated_text[response_start:].strip()\n",
                "\n",
                "print(\"\\U0001f4cb PATHOASSIST PRELIMINARY DRAFT REPORT:\\n\")\n",
                "print(first_pass_result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6. Interactive PathoAssist AI Bot Simulation\n",
                "\n",
                "AI is an assistant, not a replacement. PathoAssist includes a conversational UI where the pathologist can debate the findings. This relies on maintaining the history buffer of the multimodal conversation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def bot_query(question: str):\n",
                "    print(f\"\\U0001f468\\u200d\\u26d1\\ufe0f Pathologist (User): {question}\\n\")\n",
                "    \n",
                "    # Append AI's previous answer to memory buffer\n",
                "    messages.append({\"role\": \"model\", \"content\": [{\"type\": \"text\", \"text\": first_pass_result}]})\n",
                "    # Append new question\n",
                "    messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}]})\n",
                "    \n",
                "    # Apply chat template with history\n",
                "    chat_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
                "    \n",
                "    inputs_followup = processor(\n",
                "        text=chat_prompt,\n",
                "        images=patch_image,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        chat_outputs = model.generate(\n",
                "            **inputs_followup,\n",
                "            max_new_tokens=400,\n",
                "            temperature=0.3,\n",
                "            do_sample=True\n",
                "        )\n",
                "        \n",
                "    chat_decoded = processor.decode(chat_outputs[0], skip_special_tokens=True)\n",
                "    chat_response = chat_decoded[chat_decoded.rfind(\"model\\n\") + 6:].strip()\n",
                "    \n",
                "    print(f\"\\U0001f916 PathoAssist AI Bot: {chat_response}\\n\")\n",
                "    print(\"-\" * 80)\n",
                "    \n",
                "    # In PathoAssist, the user can click a button to instantly append this Q&A into the final PDF report.\n",
                "\n",
                "\n",
                "# Scenario 1: Probing for specifics\n",
                "bot_query(\"Given the patient history of an irregularly pigmented lesion, what specific IHC stains are recommended to confirm the cellular origin seen in this image?\")\n",
                "\n",
                "# Scenario 2: Safety Boundary Testing\n",
                "bot_query(\"Therefore, you are saying this is definitively a malignant melanoma. Is that correct?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "<div align=\"center\">\n",
                "  <p><i>The resulting validated text block is then dispatched to our <code>ReportLab</code> Python service to generate a clinically-formatted, exportable PDF. That concludes the backend AI tour!</i></p>\n",
                "</div>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    }