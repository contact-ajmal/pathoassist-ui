{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PathoAssist: Reproducible MedGemma Inference\n",
                "\n",
                "This notebook provides the core reproducible code for the **PathoAssist** multimodal inference engine. It demonstrates how we utilize **Google MedGemma (2B/4B)** to process a high-variance Region of Interest (ROI) from a Whole Slide Image, inject Electronic Health Record (EHR) text context, and generate an explainable clinical finding.\n",
                "\n",
                "**Note:** To run this notebook, you will need a Hugging Face account with access granted to the `google/medgemma-2b` model. You can run this locally (if you have MPS/CUDA) or directly in Google Colab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & Authentication"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Install required dependencies\n",
                "!pip install -q git+https://github.com/huggingface/transformers.git accelerate bitsandbytes\n",
                "!pip install -q Pillow requests\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import requests\n",
                "from transformers import AutoProcessor, AutoModelForCausalLM\n",
                "\n",
                "# Authenticate with HuggingFace (Required to download MedGemma weights)\n",
                "from huggingface_hub import notebook_login\n",
                "\n",
                "print(\"Please login with your Hugging Face token (make sure you have accepted the MedGemma terms of use):\")\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Initialization (PathoAssist Local Backend Simulation)\n",
                "\n",
                "Here we load the model. In our production application, we use `bitsandbytes` to load this in 4-bit or 8-bit precision, allowing the model to run on edge devices like basic MacBooks and consumer PCs without cloud APIs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Set device based on available hardware (CUDA for NVIDIA, MPS for Apple Silicon, CPU fallback)\n",
                "if torch.cuda.is_available():\n",
                "    device = \"cuda\"\n",
                "elif torch.backends.mps.is_available():\n",
                "    device = \"mps\"\n",
                "else:\n",
                "    device = \"cpu\"\n",
                "    \n",
                "print(f\"Loading models on {device}...\")\n",
                "\n",
                "model_id = \"google/medgemma-2b\"\n",
                "\n",
                "# Load processor\n",
                "processor = AutoProcessor.from_pretrained(model_id)\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    device_map=device,\n",
                "    torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
                ")\n",
                "model.eval() # Set to evaluation mode\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Clinical RAG & Multimodal Context Processing\n",
                "\n",
                "A major innovation in PathoAssist is that we don't just classify an image. We use **Clinical Retrieval-Augmented Generation (RAG)** to inject the patient's history directly into the instruction payload. This allows MedGemma to perform context-aware reasoning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Simulated Patient Data (From the PathoAssist UI)\n",
                "patient_context = {\n",
                "    \"age\": 64,\n",
                "    \"gender\": \"Male\",\n",
                "    \"history\": \"Patient presents with an irregularly pigmented lesion on the left forearm, changing in size over 6 months.\",\n",
                "    \"tissue_type\": \"Skin excision\",\n",
                "    \"stain\": \"H&E\"\n",
                "}\n",
                "\n",
                "def build_clinical_prompt(context: dict) -> str:\n",
                "    \"\"\"\n",
                "    Constructs the specialized prompt enforcing the 'Safety Layer' \n",
                "    and objective histopathological description.\n",
                "    \"\"\"\n",
                "    return f\"\"\"\n",
                "    You are an expert, highly meticulous pathologist analyzing an H&E stained pathology patch.\n",
                "    \n",
                "    PATIENT CLINICAL CONTEXT:\n",
                "    - Demographics: {context.get('age')}yo {context.get('gender')}\n",
                "    - Clinical History: {context.get('history')}\n",
                "    - Tissue Origin: {context.get('tissue_type')}\n",
                "    \n",
                "    TASK:\n",
                "    Analyze this image tile and write a structured, objective morphological description. \n",
                "    Describe the cellular architecture, nuclear features (e.g., pleomorphism, nucleoli), \n",
                "    and any signs of invasion or mitotic activity. \n",
                "    \n",
                "    RULES:\n",
                "    1. Be descriptive and analytical.\n",
                "    2. Do NOT make a definitive final diagnosis. Use phrasing like \"Features are highly suggestive of...\" \n",
                "    or \"Findings are consistent with...\". \n",
                "    3. End with a recommendation for next steps (e.g., specific IHC stains).\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execution Pipeline (Simulating the PathoAssist Inference Engine)\n",
                "\n",
                "Here we execute the core multimodal inference. We load a sample high-variance histopathology patch (which PathoAssist would normally extract via OpenCV) and pass both the image and the text context to MedGemma."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load a sample open-source histopathology tile (simulating an OpenCV ROI extraction)\n",
                "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Nodular_melanoma_%282%29.jpg/640px-Nodular_melanoma_%282%29.jpg\"\n",
                "try:\n",
                "    # Standardize image size for the Vision Encoder\n",
                "    patch_image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\").resize((224, 224))\n",
                "except Exception as e:\n",
                "    print(\"Error loading sample image, please provide a local image path instead.\")\n",
                "\n",
                "# Display the ROI patch to the user\n",
                "display(patch_image)\n",
                "\n",
                "# Construct the final prompt\n",
                "prompt_text = build_clinical_prompt(patient_context)\n",
                "\n",
                "# Format the message using the MedGemma chat template format\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"type\": \"image\"},\n",
                "            {\"type\": \"text\", \"text\": prompt_text}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "# Apply chat template\n",
                "final_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
                "\n",
                "# Tokenize inputs and move to hardware accelerator\n",
                "inputs = processor(\n",
                "    text=final_prompt,\n",
                "    images=patch_image,\n",
                "    return_tensors=\"pt\"\n",
                ").to(device)\n",
                "\n",
                "print(\"\\n--- Starting MedGemma Inference (PathoAssist Engine) ---\\n\")\n",
                "\n",
                "# Generate findings. Low temperature is critical for clinical factuality.\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=512,\n",
                "        temperature=0.2,\n",
                "        do_sample=True\n",
                "    )\n",
                "\n",
                "# Decode the generated text\n",
                "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Extract only the AI's response (removing the prompt input structure)\n",
                "response_start = generated_text.rfind(\"model\\n\") + 6\n",
                "inference_result = generated_text[response_start:].strip()\n",
                "\n",
                "print(\"PathoAssist Generated Report:\\n\")\n",
                "print(inference_result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. PathoCopilot Interactive Debate (Context Retention)\n",
                "PathoAssist allows the physician to converse with the model to debate the findings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Simulate the pathologist asking a follow-up query based on the the previous findings\n",
                "followup_question = \"Given the findings and the patient's history, what specific IHC stains would you prioritize to confirm the cellular origin?\"\n",
                "\n",
                "print(f\"Pathologist (Copilot Input): {followup_question}\\n\")\n",
                "\n",
                "# Construct the conversation history to maintain context\n",
                "messages.append({\"role\": \"model\", \"content\": [{\"type\": \"text\", \"text\": inference_result}]})\n",
                "messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": followup_question}]})\n",
                "\n",
                "chat_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
                "\n",
                "inputs_followup = processor(\n",
                "    text=chat_prompt,\n",
                "    images=patch_image,\n",
                "    return_tensors=\"pt\"\n",
                ").to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    chat_outputs = model.generate(\n",
                "        **inputs_followup,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.2,\n",
                "        do_sample=True\n",
                "    )\n",
                "\n",
                "chat_decoded = processor.decode(chat_outputs[0], skip_special_tokens=True)\n",
                "chat_response = chat_decoded[chat_decoded.rfind(\"model\\n\") + 6:].strip()\n",
                "\n",
                "print(\"PathoCopilot Response:\\n\")\n",
                "print(chat_response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    }